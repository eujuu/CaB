{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 20:24:49.799367  2604 deprecation_wrapper.py:119] From C:\\Users\\IVPL-D11\\models\\research\\object_detection\\utils\\label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pykinect2 import PyKinectV2\n",
    "from pykinect2.PyKinectV2 import *\n",
    "from pykinect2 import PyKinectRuntime\n",
    "from acquisitionKinect import AcquisitionKinect\n",
    "from frame import Frame as Frame\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ctypes\n",
    "import _ctypes\n",
    "import sys\n",
    "import face_recognition\n",
    "import os\n",
    "from scipy import io\n",
    "import math\n",
    "#Load Basic Setting for Kinect\n",
    "pykinect = PyKinectRuntime.PyKinectRuntime(PyKinectV2.FrameSourceTypes_Depth)\n",
    "Kinect = AcquisitionKinect()\n",
    "frame = Frame()\n",
    "\n",
    "# Functions for Action Recognition\n",
    "D = 1500\n",
    "numActivities = 17\n",
    "MAXL = 30\n",
    "compareAngle = 0.5\n",
    "feature = np.zeros((20,3))\n",
    "feature_d = []\n",
    "i = 1\n",
    "framecheck = 0\n",
    "    \n",
    "frameset = np.array([])\n",
    "norm_frameset = np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingforAction():\n",
    "    filename = 'D1500.mat'\n",
    "    AM = io.loadmat(filename, mat_dtype = False)\n",
    "\n",
    "    CiM_o = AM['Cim_out']\n",
    "\n",
    "    CiM = np.array([])\n",
    "    for i in range(30):\n",
    "        CiM = np.concatenate((CiM,np.array(CiM_o)['value'][0,0][0][int(i)][0]))\n",
    "    CiM = CiM.reshape((30, D))\n",
    "\n",
    "    iMaxis_o = AM['IMaxis_out']\n",
    "    iMaxis = np.array([])\n",
    "    for i in range(3):\n",
    "        iMaxis = np.concatenate((iMaxis,np.array(iMaxis_o)['value'][0,0][0][int(i)][0]))\n",
    "    iMaxis = iMaxis.reshape((3, D))\n",
    "    \n",
    "    iMjoints_o = AM['IMjoints_out']\n",
    "    iMjoints = np.array([])\n",
    "    for i in range(14):\n",
    "        iMjoints = np.concatenate((iMjoints,np.array(iMjoints_o)['value'][0,0][0][int(i)][0]))\n",
    "    iMjoints = iMjoints.reshape((14, D))\n",
    "    \n",
    "    AM_o = AM['AM2str']\n",
    "    AMM = np.array([])\n",
    "    for i in range(3):\n",
    "        AMM = np.concatenate((AMM,np.array(AM_o)['value'][0,0][0][int(i)][0]))\n",
    "    AMM = AMM.reshape((3, D))\n",
    "    \n",
    "    return CiM, iMaxis, iMjoints, AMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosAngle (u, v):\n",
    "    cosAngle = np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))\n",
    "    return cosAngle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizeHV (v):\n",
    "    threshold = 0\n",
    "    for i in range(len (v)):\n",
    "        if v [i] > threshold:\n",
    "            v [i] = 1\n",
    "        else:\n",
    "            v [i] = -1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(data, bins):\n",
    "    split = np.array_split(np.sort(data), bins)\n",
    "    cutoffs = [x[-1] for x in split]\n",
    "    cutoffs = cutoffs[:-1]\n",
    "    discrete = np.digitize(data, cutoffs, right=True)\n",
    "    return discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdcResult(AM,M0,CiM,iMjoints,iMaxis,MAXL,compareAngle):\n",
    "    TP = np.zeros((1,17))\n",
    "    FN = np.zeros((1,17))\n",
    "    FP = np.zeros((1,17))\n",
    "    predictedLabel = -1\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    print(\"in\")\n",
    "    for j in range (3):\n",
    "        for i in range(1, 4):\n",
    "            if(i == 1 or i == 2 or i == 3 or i == 4 or i == 5 or i == 6 or i == 9 or i == 10 or i == 11 or i == 12 or i == 16):\n",
    "                tmp = np.zeros(D)\n",
    "                M = discretize(M0, MAXL)\n",
    "                M = M.reshape((50, 42))\n",
    "                for k in range( len(M)):\n",
    "                    \n",
    "                    for n in range(len(M[0])):\n",
    "                        \n",
    "                        jointHDC = iMjoints[math.floor(n/3)]\n",
    "    \n",
    "                        axisHDC = iMaxis[n%3]\n",
    "                        \n",
    "                        valueHDC = CiM[M[k][n]]\n",
    "                       #\n",
    "                        tmp += ((jointHDC*axisHDC) * valueHDC)\n",
    "                    maxAngle = compareAngle\n",
    "                    for b in range (1,4):\n",
    "                        \n",
    "                        if(b == 1 or b == 2 or b == 3 or b == 4 or b == 5 or b == 6 or b == 9 or b == 10 or b == 11 or b == 12 or b == 16):\n",
    "                            angle = cosAngle (AM[b-1], binarizeHV(tmp))\n",
    "                            \n",
    "                            if (angle > maxAngle):\n",
    "                                maxAngle = angle\n",
    "                                predictedLabel = b\n",
    "                    #if predictedLabel == i:\n",
    "                        #TP(:,i) = TP(:,i)+1\n",
    "                    #else:\n",
    "                        #FN(:,i) = FN(:,i)+1;\n",
    "                        #FP(:,predictedLabel) = FP(:,predictedLabel)+1\n",
    "   #print(\"predictLabel:\", predictedLabel)\n",
    "\n",
    "    #precision = TP./(TP+FP)\n",
    "    #recall = TP./(TP+FN)\n",
    "    return predictedLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting for Face Recognition\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Load sample pictures and learn how to recognize it.\n",
    "dirname = 'face_reco/knowns'\n",
    "files = os.listdir(dirname)\n",
    "for filename in files:\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    if ext == '.jpg':\n",
    "        known_face_names.append(name)\n",
    "        pathname = os.path.join(dirname, filename)\n",
    "        img = face_recognition.load_image_file(pathname)\n",
    "        face_encoding = face_recognition.face_encodings(img)[0]\n",
    "        known_face_encodings.append(face_encoding)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IVPL-D11\\Anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:208: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SoraRyu', 96.36363636363636]]\n",
      "[]\n",
      "[]\n",
      "[['laptop', 53.0]]\n",
      "[['SoraRyu', 82.57851239669421]]\n",
      "[['SoraRyu', 82.85123966942149]]\n",
      "[['SoraRyu', 86.29752066115702]]\n",
      "[['SoraRyu', 87.0]]\n",
      "[['SoraRyu', 86.54545454545455]]\n",
      "[['SoraRyu', 86.0]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[['SoraRyu', 89.34710743801652]]\n",
      "[]\n",
      "[]\n",
      "[['SoraRyu', 87.75206611570248]]\n",
      "[['SoraRyu', 98.55371900826447]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[['SoraRyu', 51.735537190082646], ['person', 54.0], ['laptop', 54.14876033057851]]\n",
      "[['laptop', 54.19834710743802], ['SoraRyu', 98.14876033057851]]\n",
      "[['person', 52.0], ['laptop', 52.96694214876033], ['SoraRyu', 87.85950413223141]]\n",
      "[['SoraRyu', 50.0], ['person', 52.099173553719005], ['laptop', 53.0]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[['laptop', 53.04132231404959]]\n",
      "[['laptop', 53.0], ['SoraRyu', 80.56198347107438]]\n",
      "[['laptop', 53.02479338842975], ['SoraRyu', 102.91735537190083]]\n",
      "[['laptop', 48.413223140495866], ['SoraRyu', 72.09917355371901]]\n",
      "[['laptop', 43.95867768595041], ['SoraRyu', 78.69421487603306]]\n",
      "[['laptop', 44.082644628099175], ['person', 70.79338842975207], ['SoraRyu', 194.04132231404958]]\n",
      "find 0\n",
      "[['laptop', 38.16528925619835], ['person', 63.97520661157025], ['SoraRyu', 90.59504132231405]]\n",
      "find 1\n",
      "[['laptop', 30.03305785123967], ['SoraRyu', 57.760330578512395], ['person', 130.31404958677686]]\n",
      "find 2\n",
      "[['laptop', 30.950413223140497], ['person', 58.0], ['person', 167.7603305785124]]\n",
      "find 3\n",
      "[['laptop', 30.429752066115704], ['person', 60.35537190082645], ['person', 164.62809917355372]]\n",
      "find 4\n",
      "[['laptop', 30.702479338842974], ['SoraRyu', 60.760330578512395], ['person', 178.05785123966942]]\n",
      "find 5\n",
      "[['laptop', 29.950413223140497], ['SoraRyu', 59.760330578512395]]\n",
      "find 6\n",
      "[['laptop', 30.454545454545453], ['SoraRyu', 59.0]]\n",
      "find 7\n",
      "[['laptop', 31.0], ['SoraRyu', 61.86776859504132]]\n",
      "find 8\n",
      "[['laptop', 34.0], ['SoraRyu', 72.0]]\n",
      "find 9\n",
      "[['laptop', 41.0]]\n",
      "find 10\n",
      "[['laptop', 44.049586776859506], ['SoraRyu', 84.70247933884298]]\n",
      "find 11\n",
      "[['laptop', 48.80165289256198], ['SoraRyu', 83.33884297520662]]\n",
      "find 12\n",
      "[['laptop', 49.0], ['person', 90.23140495867769], ['SoraRyu', 92.6694214876033]]\n",
      "find 13\n",
      "[['laptop', 47.950413223140494], ['SoraRyu', 84.0]]\n",
      "find 14\n",
      "[['laptop', 47.49586776859504], ['SoraRyu', 84.0]]\n",
      "find 15\n",
      "[['laptop', 47.68595041322314], ['SoraRyu', 83.0]]\n",
      "find 16\n",
      "[['laptop', 47.28099173553719], ['SoraRyu', 83.0]]\n",
      "find 17\n",
      "[['laptop', 47.603305785123965], ['SoraRyu', 83.21487603305785]]\n",
      "find 18\n",
      "[['laptop', 47.12396694214876], ['SoraRyu', 83.39669421487604]]\n",
      "find 19\n",
      "[['laptop', 47.66115702479339], ['SoraRyu', 83.41322314049587]]\n",
      "find 20\n",
      "[['laptop', 47.06611570247934], ['SoraRyu', 83.5702479338843]]\n",
      "find 21\n",
      "[['laptop', 47.553719008264466], ['SoraRyu', 83.74380165289256]]\n",
      "find 22\n",
      "[['laptop', 47.87603305785124], ['SoraRyu', 83.94214876033058]]\n",
      "find 23\n",
      "[['laptop', 47.553719008264466], ['SoraRyu', 83.66115702479338]]\n",
      "find 24\n",
      "[['laptop', 47.578512396694215], ['SoraRyu', 83.41322314049587]]\n",
      "find 25\n",
      "[['laptop', 47.66942148760331], ['SoraRyu', 83.45454545454545]]\n",
      "find 26\n",
      "[['laptop', 47.32231404958678], ['SoraRyu', 83.18181818181819]]\n",
      "find 27\n",
      "[['laptop', 47.11570247933884], ['SoraRyu', 83.08264462809917]]\n",
      "find 28\n",
      "[['laptop', 47.64462809917355], ['SoraRyu', 83.2892561983471]]\n",
      "find 29\n",
      "[['laptop', 47.81818181818182], ['SoraRyu', 83.56198347107438]]\n",
      "find 30\n",
      "[['laptop', 47.735537190082646], ['SoraRyu', 83.78512396694215]]\n",
      "find 31\n",
      "[['laptop', 47.239669421487605], ['SoraRyu', 84.0]]\n",
      "find 32\n",
      "[['laptop', 47.231404958677686], ['SoraRyu', 83.52892561983471]]\n",
      "find 33\n",
      "[['laptop', 47.950413223140494], ['SoraRyu', 83.32231404958678]]\n",
      "find 34\n",
      "[['laptop', 47.77685950413223], ['SoraRyu', 83.76859504132231]]\n",
      "find 35\n",
      "[['laptop', 47.53719008264463], ['SoraRyu', 83.2892561983471]]\n",
      "find 36\n",
      "[['laptop', 47.32231404958678], ['SoraRyu', 83.20661157024793]]\n",
      "find 37\n",
      "[['laptop', 47.63636363636363]]\n",
      "find 38\n",
      "[['laptop', 47.87603305785124], ['SoraRyu', 83.57851239669421]]\n",
      "find 39\n",
      "[['laptop', 47.429752066115704], ['SoraRyu', 83.93388429752066]]\n",
      "find 40\n",
      "[['laptop', 47.45454545454545], ['SoraRyu', 83.66115702479338]]\n",
      "find 41\n",
      "[['laptop', 47.83471074380165], ['SoraRyu', 83.65289256198348]]\n",
      "find 42\n",
      "[['laptop', 47.768595041322314], ['SoraRyu', 83.17355371900827]]\n",
      "find 43\n",
      "[['laptop', 47.35537190082645], ['SoraRyu', 83.93388429752066]]\n",
      "find 44\n",
      "[['laptop', 47.84297520661157], ['SoraRyu', 83.44628099173553]]\n",
      "find 45\n",
      "[['laptop', 47.20661157024794], ['SoraRyu', 83.70247933884298]]\n",
      "find 46\n",
      "[['laptop', 47.29752066115702], ['SoraRyu', 83.3801652892562]]\n",
      "find 47\n",
      "[['laptop', 47.02479338842975], ['SoraRyu', 83.64462809917356]]\n",
      "find 48\n",
      "[['laptop', 47.743801652892564], ['SoraRyu', 83.91735537190083]]\n",
      "find 49\n",
      "in\n",
      "Action Detect----------------------------------------------------------\n",
      "sit\n",
      "-----------------------------------------------------------------------\n",
      "hiiiiiiiiiiiiiiiiiiiiiii\n",
      "index:  1\n",
      "eng:  SoraRyu is sitting  behind laptop.\n",
      "ko:  SoraRyu는 노트북 뒤에 앉아 있습니다.\n",
      "[['laptop', 47.52892561983471], ['SoraRyu', 84.0]]\n",
      "find 0\n",
      "[['laptop', 48.20661157024794], ['SoraRyu', 83.85123966942149]]\n",
      "find 1\n",
      "[['laptop', 47.925619834710744], ['SoraRyu', 83.87603305785125]]\n",
      "find 2\n",
      "[['laptop', 47.36363636363637], ['SoraRyu', 84.0]]\n",
      "find 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8d63ce0539ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtmp_situation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# --- Getting frames and drawing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mKinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mKinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_color_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mimage_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_color_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\models\\research\\object_detection\\acquisitionKinect.py\u001b[0m in \u001b[0;36mget_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquireFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\models\\research\\object_detection\\acquisitionKinect.py\u001b[0m in \u001b[0;36macquireFrame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_new_depth_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_depth_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_new_color_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\models\\research\\object_detection\\acquisitionKinect.py\u001b[0m in \u001b[0;36mget_depth_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_frameDepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_kinect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_depth_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_frameDepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_frameDepth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m424\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_frameDepthQuantized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_frameDepth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m8.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_color_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "CiM, iMaxis, iMjoints, AMM = loadingforAction()\n",
    "flag = 0\n",
    "framecheck = 0\n",
    "#Object Detection\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    while True:\n",
    "        peop = 0\n",
    "        people = []\n",
    "        situation=\"\"\n",
    "        tmp_situation = \"\"\n",
    "        # --- Getting frames and drawing\n",
    "        Kinect.get_frame(frame)\n",
    "        Kinect.get_color_frame()\n",
    "        image_np = Kinect._kinect.get_last_color_frame()\n",
    "        #image_np = Kinect._frameRGB\n",
    "        image_depth = Kinect._frameDepthQuantized \n",
    "        Skeleton_img = Kinect._frameSkeleton\n",
    "        image_np = np.reshape(image_np, (Kinect._kinect.color_frame_desc.Height, Kinect._kinect.color_frame_desc.Width, 4))\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)\n",
    "        show_img = image_np\n",
    "       # show_img = image_np[ 200:1020, 350:1780]\n",
    "        show_img = cv2.resize(show_img, (512,424))\n",
    "        \n",
    "        #image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)\n",
    "        #image_np = image_np[ 200:1020, 350:1780]\n",
    "        #image_np = cv2.resize(image_np, (512,424))\n",
    "        \n",
    "\n",
    "        rgb_small_frame = cv2.resize(image_np, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "# Face recognition\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                min_value = min(distances)\n",
    "\n",
    "\n",
    "                name = \"Unknown\"\n",
    "                if min_value < 0.9:\n",
    "                    index = np.argmin(distances)\n",
    "                    name = known_face_names[index]\n",
    "\n",
    "                face_names.append(name)\n",
    "\n",
    "        process_this_frame = not process_this_frame\n",
    "\n",
    "       \n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(image_np, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(image_np, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(image_np, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "            \n",
    "#---------------- Object Detection\n",
    "\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    " \n",
    "        (boxes, scores, classes, num) = sess.run(\n",
    "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "            feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "\n",
    "\n",
    "        coordinates = vis_util.return_coordinates(\n",
    "                    image_np,\n",
    "                    np.squeeze(boxes),\n",
    "                    np.squeeze(classes).astype(np.int32),\n",
    "                    np.squeeze(scores),\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    line_thickness=8,\n",
    "                    min_score_thresh=0.5)\n",
    "        \"\"\"\n",
    "                     Pixel_Depth_Dict = {}\n",
    "                Pixel_Depth_Sum = 0\n",
    "                non_error_count = 0\n",
    "                for i in range(len(coordinates)):\n",
    "                        \n",
    "                    # ----- Depth Recognition\n",
    "                    for x in range(int(coordinates[i][3]), int(coordinates[i][4]+1)):\n",
    "                        for y in range(int(coordinates[i][1]), int(coordinates[i][2]+1)):\n",
    "                            Pixel_Depth_Sum += frameD[((y * 512) + x)]\n",
    "                            if frameD[((y * 512) + x)] != 0:\n",
    "                                non_error_count += 1\n",
    "                    \n",
    "                    Pixel_Depth_Dict[coordinates[i][0]] = int(Pixel_Depth_Sum/non_error_count)\n",
    "                \n",
    "                # Compare each objects' depth. (Sorting: close -> far)\n",
    "                sorted(Pixel_Depth_Dict.items(), key=lambda t : t[1])\n",
    "                print(Pixel_Depth_Dict)\n",
    "           \"\"\"\n",
    "        if coordinates is not None:\n",
    "            depth_arr = [[0 for x in range(2)] for y in range(len(coordinates))]\n",
    "            real_depth_arr = [[0 for x in range(2)] for y in range(len(coordinates))]\n",
    "            count = 0\n",
    "            for i in range(len(coordinates)):\n",
    "                if coordinates[i][0] == 'person':\n",
    "                    flag = 1\n",
    "                    if count < len(face_names):\n",
    "                        coordinates[i][0] = face_names[count]\n",
    "                        count += 1\n",
    "                        \n",
    "            #print(coordinates)\n",
    "            xpoint_for_depth=[]\n",
    "            ypoint_for_depth=[]\n",
    "            \"\"\"\n",
    "            Pixel_Depth_Dict = {}\n",
    "            Pixel_Depth_Sum = 0\n",
    "            non_error_count = 0\n",
    "            for i in range(len(coordinates)):\n",
    "                        \n",
    "                    # ----- Depth Recognition\n",
    "                for x in range(int(coordinates[i][3]), int(coordinates[i][4]+1)):\n",
    "                    for y in range(int(coordinates[i][1]), int(coordinates[i][2]+1)):\n",
    "                        Pixel_Depth_Sum += image_depth[((y * 512) + x)]\n",
    "                        if image_depth[((y * 512) + x)] != 0:\n",
    "                            non_error_count += 1\n",
    "                    \n",
    "                Pixel_Depth_Dict[coordinates[i][0]] = int(Pixel_Depth_Sum/non_error_count)\n",
    "                \n",
    "                # Compare each objects' depth. (Sorting: close -> far)\n",
    "            sorted(Pixel_Depth_Dict.items(), key=lambda t : t[1])\n",
    "            print(Pixel_Depth_Dict)\n",
    "            \"\"\"\n",
    "\n",
    "            #print(\"object 개수:\", len(coordinates))\n",
    "            for i in range(len(coordinates)):\n",
    "                ypoint_for_depth.append(int((coordinates[i][1]+coordinates[i][2])/2))\n",
    "                xpoint_for_depth.append(int((coordinates[i][3]+coordinates[i][4])/2))\n",
    "\n",
    "            #print(\"ypoint_for_depth:\", ypoint_for_depth)\n",
    "            #print(\"xpoint_for_depth:\", xpoint_for_depth)\n",
    "            m = 0\n",
    "            for i in range(len(coordinates)):\n",
    "                if(coordinates[i][0]!=\"N/A\"):\n",
    "                    depth_arr[m][0] = coordinates[i][0]\n",
    "                    depth_arr[m][1] = 0\n",
    "                    for x in range (-5, 6):\n",
    "                        for y in range(-5, 6):\n",
    "                            depth_arr[m][1] += image_depth[int((ypoint_for_depth[i]+x) * 424 / 1080)][int((xpoint_for_depth[i]+y) * 512 / 1920)]\n",
    "                            show_img[int((ypoint_for_depth[i]+x) * 424 / 1080)][int((xpoint_for_depth[i]+y) * 512 / 1920)] = [255, 0,95]\n",
    "                    depth_arr[m][1] /= 121\n",
    "                    m+=1\n",
    "\n",
    "            # Compare each objects' depth. (Sorting: close -> far)\n",
    "            depth_arr.sort(key=lambda x:x[1])\n",
    "            print(depth_arr)\n",
    "       \n",
    "               # print(coordinates[i][0], \"'s depth: \", Pixel_Depth)\n",
    "               \n",
    "        cv2.imshow(\"ee\", image_np)  \n",
    "                \n",
    "        #cv2.imshow(\"depthh\", show_img)\n",
    "        #cv2.imshow('KINECT Color', image_np)\n",
    "        cv2.imshow(\"depth\", image_depth)\n",
    "#Action Recognition\n",
    "        max_people = -1\n",
    "        max_depth = -1\n",
    "        if Skeleton_img is not None:\n",
    "\n",
    "            \n",
    "            cv2.resize(Skeleton_img, (640, 480))\n",
    "            print(\"find\", framecheck)            \n",
    "           \n",
    "            feature[1][0] = Kinect.joint_points3D[3].x\n",
    "            feature[1][1] = Kinect.joint_points3D[3].y\n",
    "            feature[1][2] = Kinect.joint_points3D[3].z\n",
    "            \n",
    "            feature[2][0] = Kinect.joint_points3D[2].x\n",
    "            feature[2][1] = Kinect.joint_points3D[2].y\n",
    "            feature[2][2] = Kinect.joint_points3D[2].z\n",
    "            \n",
    "            feature[3][0] = Kinect.joint_points3D[1].x\n",
    "            feature[3][1] = Kinect.joint_points3D[1].y\n",
    "            feature[3][2] = Kinect.joint_points3D[1].z\n",
    "\n",
    "            feature[4][0] = Kinect.joint_points3D[8].x\n",
    "            feature[4][1] = Kinect.joint_points3D[8].y\n",
    "            feature[4][2] = Kinect.joint_points3D[8].z\n",
    "\n",
    "            feature[5][0] = Kinect.joint_points3D[9].x\n",
    "            feature[5][1] = Kinect.joint_points3D[9].y\n",
    "            feature[5][2] = Kinect.joint_points3D[9].z\n",
    "\n",
    "            feature[6][0] = Kinect.joint_points3D[4].x\n",
    "            feature[6][1] = Kinect.joint_points3D[4].y\n",
    "            feature[6][2] = Kinect.joint_points3D[4].z\n",
    "\n",
    "            feature[7][0] = Kinect.joint_points3D[5].x\n",
    "            feature[7][1] = Kinect.joint_points3D[5].y\n",
    "            feature[7][2] = Kinect.joint_points3D[5].z\n",
    "\n",
    "            feature[8][0] = Kinect.joint_points3D[16].x\n",
    "            feature[8][1] = Kinect.joint_points3D[16].y\n",
    "            feature[8][2] = Kinect.joint_points3D[16].z\n",
    "\n",
    "            feature[9][0] = Kinect.joint_points3D[17].x\n",
    "            feature[9][1] = Kinect.joint_points3D[17].y\n",
    "            feature[9][2] = Kinect.joint_points3D[17].z\n",
    "\n",
    "            feature[10][0] = Kinect.joint_points3D[12].x\n",
    "            feature[10][1] = Kinect.joint_points3D[12].y\n",
    "            feature[10][2] = Kinect.joint_points3D[12].z\n",
    "\n",
    "            feature[11][0] = Kinect.joint_points3D[13].x\n",
    "            feature[11][1] = Kinect.joint_points3D[13].y\n",
    "            feature[11][2] = Kinect.joint_points3D[13].z\n",
    "\n",
    "            feature[12][0] = Kinect.joint_points3D[11].x\n",
    "            feature[12][1] = Kinect.joint_points3D[11].y\n",
    "            feature[12][2] = Kinect.joint_points3D[11].z\n",
    "\n",
    "            feature[13][0] = Kinect.joint_points3D[7].x\n",
    "            feature[13][1] = Kinect.joint_points3D[7].y\n",
    "            feature[13][2] = Kinect.joint_points3D[7].z\n",
    "\n",
    "            feature[14][0] = Kinect.joint_points3D[19].x\n",
    "            feature[14][1] = Kinect.joint_points3D[19].y\n",
    "            feature[14][2] = Kinect.joint_points3D[19].z\n",
    "\n",
    "            feature[15][0] = Kinect.joint_points3D[15].x\n",
    "            feature[15][1] = Kinect.joint_points3D[15].y\n",
    "            feature[15][2] = Kinect.joint_points3D[15].z\n",
    "            tmp = []\n",
    "            for i in range (1, 16):\n",
    "                for j in range(3):\n",
    "                    tmp.append(feature[i][j])\n",
    "\n",
    "            frameset = np.concatenate((frameset, tmp), axis = 0)\n",
    "            framecheck+=1\n",
    "            if(framecheck == 50):\n",
    "                frameset = frameset.reshape((framecheck, 45))\n",
    "                for row in range(len(frameset)):\n",
    "                    torsox = float(frameset[row][6])\n",
    "                    torsoy = float(frameset[row][7])\n",
    "                    torsoz = float(frameset[row][8])\n",
    "                    neckx = float(frameset[row][3])\n",
    "                    necky = float(frameset[row][4])\n",
    "                    neckz = float(frameset[row][5])\n",
    "                    denom = math.sqrt(math.pow((neckx - torsox),2) + math.pow((necky - torsoy),2) + math.pow((neckz - torsoz),2))\n",
    "                    tmp = []\n",
    "                    i = 0\n",
    "                    while (i < 6):\n",
    "                        tmp.append((float(frameset[row][i]) - torsox)/denom)\n",
    "                        tmp.append((float(frameset[row][i+1]) - torsoy)/denom)\n",
    "                        tmp.append((float(frameset[row][i+2]) - torsoz)/denom)\n",
    "                        i+=3\n",
    "                    i = 9\n",
    "                    while (i < 45):\n",
    "                        tmp.append((float(frameset[row][i]) - torsox)/denom)\n",
    "                        tmp.append((float(frameset[row][i+1]) - torsoy)/denom)\n",
    "                        tmp.append((float(frameset[row][i+2]) - torsoz)/denom)\n",
    "                        i+=3\n",
    "                    norm_frameset = np.concatenate((norm_frameset, tmp), axis = 0)\n",
    "                    \n",
    "                #norm_frameset = norm_frameset.reshape((k, 42))\n",
    "               # CiM,iMjoints,iMaxis = initItemMemories (D, MAXL)\n",
    "                \n",
    "                predictedLabel = hdcResult(AMM,norm_frameset,CiM,iMjoints,iMaxis,MAXL,compareAngle)\n",
    "                print(\"Action Detect----------------------------------------------------------\")\n",
    "                if(predictedLabel == 1):\n",
    "                    action = \"standing\"\n",
    "                    print(\"still\")\n",
    "                elif (predictedLabel == 2 ):\n",
    "                    action = \"sitting\"\n",
    "                    print (\"sit\")\n",
    "                elif (predictedLabel == 3 ):\n",
    "                    action = \"talking on the phone\"\n",
    "                    print (\"phone\")\n",
    "                print(\"-----------------------------------------------------------------------\")\n",
    "               \n",
    "                tmp = []\n",
    "                frameset = np.array([])\n",
    "                norm_frameset = np.array([])\n",
    "                framecheck = 0\n",
    "                if(len(face_names)> max_people):\n",
    "                    for i in range(len(face_names)): \n",
    "                        if face_names[i] not in people:\n",
    "                            people.append( face_names[i])\n",
    "                           \n",
    "                    max_people = len(people)\n",
    "                            \n",
    "       \n",
    "                back_objects = \"\"\n",
    "                front_objects = \"\"\n",
    "                if(flag==1):\n",
    "                    if(len(people)==1): # one person\n",
    "                        if(people[0]):\n",
    "                            situation += people[0] + \" is \"\n",
    "                        else:\n",
    "                            situation += \"Unknown is \"\n",
    "                        situation += action + \" \"\n",
    "                        tmp_situation+=situation\n",
    "                        #back_objects += situation\n",
    "                        #front_objects += situation\n",
    "                        \n",
    "                        if(len(depth_arr)> max_depth):\n",
    "                            real_depth_arr = depth_arr\n",
    "                            print(\"hiiiiiiiiiiiiiiiiiiiiiii\")\n",
    "                            for i in range(len(depth_arr)):\n",
    "                                if(real_depth_arr[i][0]==face_names[0] or real_depth_arr[i][0]== \"person\"):\n",
    "                                    index = i\n",
    "                            print(\"index: \", index)\n",
    "                            \n",
    "                            if(index!= 0):\n",
    "                                for i in range(index-1):\n",
    "                                    back_objects += real_depth_arr[i][0] + \" and \"\n",
    "                                back_objects += real_depth_arr[index-1][0]\n",
    "                                tmp_situation+=\" behind \" + back_objects\n",
    "                            \n",
    "                            if(index!=len(real_depth_arr)-1):\n",
    "                                if(index!=0):\n",
    "                                    tmp_situation+= \" and \"\n",
    "                                for i in range(index+1, len(real_depth_arr)-1):\n",
    "                                    front_objects += real_depth_arr[i][0] + \" and \" \n",
    "                                front_objects+= real_depth_arr[-1][0]    \n",
    "                                situation = tmp_situation + \" in front of \" + front_objects\n",
    "                            else:\n",
    "                                situation = tmp_situation\n",
    "                            max_depth = len(real_depth_arr)\n",
    "                    \n",
    "                    elif (len(people)>=2) : # more than two people\n",
    "                        for i in range(len(people)-1):\n",
    "                            situation += people[i] + \" and \"\n",
    "                        situation += people[len(people)-1]+ \" are near by \"\n",
    "                   # if \"person\" in Pixel_Depth_Dict.keys():\n",
    "\n",
    "                        situation += action\n",
    "                    situation += \".\"\n",
    "                    print(\"eng: \", situation)\n",
    "                    word = TextBlob(situation)\n",
    "                    print(\"ko: \", word.translate(to='ko'))\n",
    "                    #print(\"eng: \", back_objects)\n",
    "                    #print(\"eng: \", front_objects)\n",
    "                    #word1 = TextBlob(back_objects)\n",
    "                    #word2 = TextBlob(front_objects)\n",
    "                    #print(\"ko: \", word1.translate(to='ko'))\n",
    "                    #print(\"ko: \", word2.translate(to='ko'))\n",
    "        i=i+1\n",
    "       # print(\"--------------------------------------------------- people: \", people)\n",
    "       # print(\"--------------------------------------------------- situation: \", situation)\n",
    "       # print(\"--------------------------------------------------- face: \", face_names)\n",
    "        \n",
    "                \n",
    "        \n",
    "\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
