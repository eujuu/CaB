{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What model to download.\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 19:19:17.931694  1616 deprecation_wrapper.py:119] From C:\\Users\\IVPL-D11\\models\\research\\object_detection\\utils\\label_map_util.py:132: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pykinect2 import PyKinectV2\n",
    "from pykinect2.PyKinectV2 import *\n",
    "from pykinect2 import PyKinectRuntime\n",
    "from acquisitionKinect import AcquisitionKinect\n",
    "from frame import Frame as Frame\n",
    "import numpy as np\n",
    "import cv2\n",
    "import ctypes\n",
    "import _ctypes\n",
    "import sys\n",
    "import face_recognition\n",
    "import os\n",
    "from scipy import io\n",
    "import math\n",
    "from gtts import gTTS\n",
    "import pyttsx3\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 145)     # setting up new voice rate\n",
    "engine.setProperty('volume',1.0)    # setting up volume level  between 0 and 1\n",
    "voices = engine.getProperty('voices')       #getting details of current voice\n",
    "engine.setProperty('voice', voices[0].id)  #changing index, changes voices.\n",
    "\n",
    "#Load Basic Setting for Kinect\n",
    "pykinect = PyKinectRuntime.PyKinectRuntime(PyKinectV2.FrameSourceTypes_Depth)\n",
    "Kinect = AcquisitionKinect()\n",
    "frame = Frame()\n",
    "\n",
    "# Functions for Action Recognition\n",
    "D = 1500\n",
    "numActivities = 17\n",
    "MAXL = 30\n",
    "compareAngle = 0.5\n",
    "feature = np.zeros((20,3))\n",
    "feature_d = []\n",
    "i = 1\n",
    "framecheck = 0\n",
    "    \n",
    "frameset = np.array([])\n",
    "norm_frameset = np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadingforAction():\n",
    "    filename = 'D1500.mat'\n",
    "    AM = io.loadmat(filename, mat_dtype = False)\n",
    "\n",
    "    CiM_o = AM['Cim_out']\n",
    "\n",
    "    CiM = np.array([])\n",
    "    for i in range(30):\n",
    "        CiM = np.concatenate((CiM,np.array(CiM_o)['value'][0,0][0][int(i)][0]))\n",
    "    CiM = CiM.reshape((30, D))\n",
    "\n",
    "    iMaxis_o = AM['IMaxis_out']\n",
    "    iMaxis = np.array([])\n",
    "    for i in range(3):\n",
    "        iMaxis = np.concatenate((iMaxis,np.array(iMaxis_o)['value'][0,0][0][int(i)][0]))\n",
    "    iMaxis = iMaxis.reshape((3, D))\n",
    "    \n",
    "    iMjoints_o = AM['IMjoints_out']\n",
    "    iMjoints = np.array([])\n",
    "    for i in range(14):\n",
    "        iMjoints = np.concatenate((iMjoints,np.array(iMjoints_o)['value'][0,0][0][int(i)][0]))\n",
    "    iMjoints = iMjoints.reshape((14, D))\n",
    "    \n",
    "    AM_o = AM['AM2str']\n",
    "    AMM = np.array([])\n",
    "    for i in range(3):\n",
    "        AMM = np.concatenate((AMM,np.array(AM_o)['value'][0,0][0][int(i)][0]))\n",
    "    AMM = AMM.reshape((3, D))\n",
    "    \n",
    "    return CiM, iMaxis, iMjoints, AMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosAngle (u, v):\n",
    "    cosAngle = np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))\n",
    "    return cosAngle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizeHV (v):\n",
    "    threshold = 0\n",
    "    for i in range(len (v)):\n",
    "        if v [i] > threshold:\n",
    "            v [i] = 1\n",
    "        else:\n",
    "            v [i] = -1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(data, bins):\n",
    "    split = np.array_split(np.sort(data), bins)\n",
    "    cutoffs = [x[-1] for x in split]\n",
    "    cutoffs = cutoffs[:-1]\n",
    "    discrete = np.digitize(data, cutoffs, right=True)\n",
    "    return discrete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdcResult(AM,M0,CiM,iMjoints,iMaxis,MAXL,compareAngle):\n",
    "    TP = np.zeros((1,17))\n",
    "    FN = np.zeros((1,17))\n",
    "    FP = np.zeros((1,17))\n",
    "    predictedLabel = -1\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    print(\"in\")\n",
    "    for j in range (3):\n",
    "        for i in range(1, 4):\n",
    "            if(i == 1 or i == 2 or i == 3 or i == 4 or i == 5 or i == 6 or i == 9 or i == 10 or i == 11 or i == 12 or i == 16):\n",
    "                tmp = np.zeros(D)\n",
    "                M = discretize(M0, MAXL)\n",
    "                M = M.reshape((30, 42))\n",
    "                for k in range( len(M)):\n",
    "                    \n",
    "                    for n in range(len(M[0])):\n",
    "                        \n",
    "                        jointHDC = iMjoints[math.floor(n/3)]\n",
    "    \n",
    "                        axisHDC = iMaxis[n%3]\n",
    "                        \n",
    "                        valueHDC = CiM[M[k][n]]\n",
    "                       #\n",
    "                        tmp += ((jointHDC*axisHDC) * valueHDC)\n",
    "                    maxAngle = compareAngle\n",
    "                    for b in range (1,4):\n",
    "                        \n",
    "                        if(b == 1 or b == 2 or b == 3 or b == 4 or b == 5 or b == 6 or b == 9 or b == 10 or b == 11 or b == 12 or b == 16):\n",
    "                            angle = cosAngle (AM[b-1], binarizeHV(tmp))\n",
    "                            \n",
    "                            if (angle > maxAngle):\n",
    "                                maxAngle = angle\n",
    "                                predictedLabel = b\n",
    "                    #if predictedLabel == i:\n",
    "                        #TP(:,i) = TP(:,i)+1\n",
    "                    #else:\n",
    "                        #FN(:,i) = FN(:,i)+1;\n",
    "                        #FP(:,predictedLabel) = FP(:,predictedLabel)+1\n",
    "   #print(\"predictLabel:\", predictedLabel)\n",
    "\n",
    "    #precision = TP./(TP+FP)\n",
    "    #recall = TP./(TP+FN)\n",
    "    return predictedLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting for Face Recognition\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Load sample pictures and learn how to recognize it.\n",
    "dirname = 'face_reco/knowns'\n",
    "files = os.listdir(dirname)\n",
    "for filename in files:\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    if ext == '.jpg':\n",
    "        known_face_names.append(name)\n",
    "        pathname = os.path.join(dirname, filename)\n",
    "        img = face_recognition.load_image_file(pathname)\n",
    "        face_encoding = face_recognition.face_encodings(img)[0]\n",
    "        known_face_encodings.append(face_encoding)\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "process_this_frame = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IVPL-D11\\Anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio.py:208: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[['person', 59.93388429752066]]\n",
      "[['person', 41.31404958677686]]\n",
      "[]\n",
      "[['chair', 78.37190082644628]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[['person', 18.801652892561982]]\n",
      "[['person', 11.768595041322314]]\n",
      "[]\n",
      "[['person', 10.644628099173554]]\n",
      "[['person', 18.05785123966942]]\n",
      "[['person', 72.36363636363636]]\n",
      "[['person', 20.446280991735538]]\n",
      "[['person', 34.0]]\n",
      "[['SoraRyu', 42.0]]\n",
      "[]\n",
      "find 0\n",
      "[]\n",
      "find 1\n",
      "[]\n",
      "find 2\n",
      "[['laptop', 25.173553719008265]]\n",
      "find 3\n",
      "[['SoraRyu', 50.87603305785124]]\n",
      "find 4\n",
      "[['laptop', 9.685950413223141], ['SoraRyu', 54.50413223140496]]\n",
      "find 5\n",
      "[['SoraRyu', 54.0]]\n",
      "find 6\n",
      "[['SoraRyu', 54.16528925619835]]\n",
      "find 7\n",
      "[['SoraRyu', 52.256198347107436]]\n",
      "find 8\n",
      "[['SoraRyu', 52.27272727272727]]\n",
      "find 9\n",
      "[['laptop', 14.0], ['SoraRyu', 51.256198347107436]]\n",
      "find 10\n",
      "[['SoraRyu', 51.32231404958678]]\n",
      "find 11\n",
      "[['laptop', 16.0], ['SoraRyu', 52.72727272727273]]\n",
      "find 12\n",
      "[['laptop', 16.0], ['SoraRyu', 52.62809917355372]]\n",
      "find 13\n",
      "[['laptop', 16.0], ['SoraRyu', 52.64462809917355]]\n",
      "find 14\n",
      "[['laptop', 16.0], ['SoraRyu', 53.96694214876033]]\n",
      "find 15\n",
      "[['laptop', 16.0], ['SoraRyu', 54.50413223140496]]\n",
      "find 16\n",
      "[['laptop', 16.0]]\n",
      "find 17\n",
      "[['laptop', 16.0], ['SoraRyu', 54.396694214876035]]\n",
      "find 18\n",
      "[['laptop', 16.0], ['SoraRyu', 54.86776859504132]]\n",
      "find 19\n",
      "[['laptop', 16.0], ['SoraRyu', 54.36363636363637]]\n",
      "find 20\n",
      "[['laptop', 16.0], ['SoraRyu', 54.429752066115704]]\n",
      "find 21\n",
      "[['laptop', 16.0], ['SoraRyu', 54.32231404958678]]\n",
      "find 22\n",
      "[['laptop', 16.0], ['SoraRyu', 54.586776859504134]]\n",
      "find 23\n",
      "[['laptop', 16.0], ['SoraRyu', 54.98347107438016]]\n",
      "find 24\n",
      "[['laptop', 16.0], ['SoraRyu', 54.35537190082645]]\n",
      "find 25\n",
      "[['laptop', 16.0], ['SoraRyu', 53.81818181818182]]\n",
      "find 26\n",
      "[['laptop', 16.0], ['SoraRyu', 54.47107438016529]]\n",
      "find 27\n",
      "[['laptop', 16.0], ['SoraRyu', 54.0]]\n",
      "find 28\n",
      "[['laptop', 16.0]]\n",
      "find 29\n",
      "in\n",
      "Action Detect----------------------------------------------------------\n",
      "sit\n",
      "-----------------------------------------------------------------------\n",
      "hiiiiiiiiiiiiiiiiiiiiiii\n",
      "index:  1\n",
      "eng:  SoraRyu is sitting  behind laptop and  in front of laptop.\n",
      "ko:  SoraRyu는 랩탑 뒤에 앉아서 랩탑 앞에 있습니다.\n",
      "[['laptop', 16.0]]\n",
      "find 0\n",
      "[['laptop', 15.884297520661157]]\n",
      "find 1\n",
      "[['laptop', 15.537190082644628]]\n",
      "find 2\n",
      "[['laptop', 15.537190082644628], ['SoraRyu', 51.14876033057851]]\n",
      "find 3\n",
      "[['laptop', 15.925619834710744]]\n",
      "find 4\n",
      "[['laptop', 16.0], ['SoraRyu', 52.3801652892562]]\n",
      "find 5\n",
      "[['laptop', 15.96694214876033], ['SoraRyu', 52.66942148760331]]\n",
      "find 6\n",
      "[['laptop', 15.900826446280991], ['SoraRyu', 52.82644628099174]]\n",
      "find 7\n",
      "[['laptop', 15.545454545454545], ['SoraRyu', 52.63636363636363]]\n",
      "find 8\n",
      "[['laptop', 15.421487603305785]]\n",
      "find 9\n",
      "[['laptop', 15.611570247933884]]\n",
      "find 10\n",
      "[['laptop', 15.950413223140496], ['SoraRyu', 52.43801652892562]]\n",
      "find 11\n",
      "[['laptop', 15.834710743801653]]\n",
      "find 12\n",
      "[['laptop', 15.925619834710744]]\n",
      "find 13\n",
      "[['laptop', 15.413223140495868], ['SoraRyu', 52.53719008264463]]\n",
      "find 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a89e466ff0b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprocess_this_frame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;31m# Find all the faces and face encodings in the current frame of video\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mface_locations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_small_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mface_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_small_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\face_recognition\\api.py\u001b[0m in \u001b[0;36mface_locations\u001b[1;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cnn\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_sentence(depth, position, index, start, end):\n",
    "    objects = \"position \"\n",
    "    for i in range(start, end):\n",
    "        objects += depth[i][0] + \" and \"\n",
    "\n",
    "                                    \n",
    "    objects += depth[end][0]\n",
    "\n",
    "    return objects\n",
    "\n",
    "CiM, iMaxis, iMjoints, AMM = loadingforAction()\n",
    "flag = 0\n",
    "framecheck = 0\n",
    "#Object Detection\n",
    "with detection_graph.as_default():\n",
    "  with tf.Session(graph=detection_graph) as sess:\n",
    "    while True:\n",
    "        peop = 0\n",
    "        people = []\n",
    "        situation=\"\"\n",
    "        tmp_situation = \"\"\n",
    "        # --- Getting frames and drawing\n",
    "        Kinect.get_frame(frame)\n",
    "        Kinect.get_color_frame()\n",
    "        image_np = Kinect._kinect.get_last_color_frame()\n",
    "        #image_np = Kinect._frameRGB\n",
    "        image_depth = Kinect._frameDepthQuantized \n",
    "        Skeleton_img = Kinect._frameSkeleton\n",
    "        image_np = np.reshape(image_np, (Kinect._kinect.color_frame_desc.Height, Kinect._kinect.color_frame_desc.Width, 4))\n",
    "        image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)\n",
    "        show_img = image_np\n",
    "       # show_img = image_np[ 200:1020, 350:1780]\n",
    "        show_img = cv2.resize(show_img, (512,424))\n",
    "        \n",
    "        #image_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)\n",
    "        #image_np = image_np[ 200:1020, 350:1780]\n",
    "        #image_np = cv2.resize(image_np, (512,424))\n",
    "        rgb_small_frame = cv2.resize(image_np, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "# Face recognition\n",
    "        if process_this_frame:\n",
    "            # Find all the faces and face encodings in the current frame of video\n",
    "            face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "            face_names = []\n",
    "            for face_encoding in face_encodings:\n",
    "                distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                min_value = min(distances)\n",
    "\n",
    "\n",
    "                name = \"Unknown\"\n",
    "                if min_value < 0.9:\n",
    "                    index = np.argmin(distances)\n",
    "                    name = known_face_names[index]\n",
    "                if name not in face_names:\n",
    "                    face_names.append(name)\n",
    "\n",
    "        process_this_frame = not process_this_frame\n",
    "\n",
    "       \n",
    "        for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "            # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "            # Draw a box around the face\n",
    "            cv2.rectangle(image_np, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "            # Draw a label with a name below the face\n",
    "            cv2.rectangle(image_np, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "            \n",
    "            font = cv2.FONT_HERSHEY_DUPLEX\n",
    "            cv2.putText(image_np, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "            \n",
    "#---------------- Object Detection\n",
    "\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    " \n",
    "        (boxes, scores, classes, num) = sess.run(\n",
    "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "            feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "\n",
    "\n",
    "        coordinates = vis_util.return_coordinates(\n",
    "                    image_np,\n",
    "                    np.squeeze(boxes),\n",
    "                    np.squeeze(classes).astype(np.int32),\n",
    "                    np.squeeze(scores),\n",
    "                    category_index,\n",
    "                    use_normalized_coordinates=True,\n",
    "                    line_thickness=8,\n",
    "                    min_score_thresh=0.5)\n",
    "        \n",
    "        if coordinates is not None:\n",
    "            depth_arr = [[0 for x in range(2)] for y in range(len(coordinates))]\n",
    "            real_depth_arr = [[0 for x in range(2)] for y in range(len(coordinates))]\n",
    "            count = 0\n",
    "            for i in range(len(coordinates)):\n",
    "                if coordinates[i][0] == 'person':\n",
    "                    flag = 1\n",
    "                    if count < len(face_names):\n",
    "                        coordinates[i][0] = face_names[count]\n",
    "                        count += 1\n",
    "                        \n",
    "            #print(coordinates)\n",
    "            xpoint_for_depth=[]\n",
    "            ypoint_for_depth=[]\n",
    "            \n",
    "            #print(\"object 개수:\", len(coordinates))\n",
    "            for i in range(len(coordinates)):\n",
    "                ypoint_for_depth.append(int((coordinates[i][1]+coordinates[i][2])/2))\n",
    "                xpoint_for_depth.append(int((coordinates[i][3]+coordinates[i][4])/2))\n",
    "\n",
    "            #print(\"ypoint_for_depth:\", ypoint_for_depth)\n",
    "            #print(\"xpoint_for_depth:\", xpoint_for_depth)\n",
    "            m = 0\n",
    "            for i in range(len(coordinates)):\n",
    "                if(coordinates[i][0]!=\"N/A\"):\n",
    "                    depth_arr[m][0] = coordinates[i][0]\n",
    "                    depth_arr[m][1] = 0\n",
    "                    for x in range (-5, 6):\n",
    "                        for y in range(-5, 6):\n",
    "                            depth_arr[m][1] += image_depth[int((ypoint_for_depth[i]+x) * 424 / 1080)][int((xpoint_for_depth[i]+y) * 512 / 1920)]\n",
    "                            show_img[int((ypoint_for_depth[i]+x) * 424 / 1080)][int((xpoint_for_depth[i]+y) * 512 / 1920)] = [255, 0,95]\n",
    "                    depth_arr[m][1] /= 121\n",
    "                    m+=1\n",
    "\n",
    "            # Compare each objects' depth. (Sorting: close -> far)\n",
    "            depth_arr.sort(key=lambda x:x[1])\n",
    "            print(depth_arr)\n",
    "       \n",
    "               # print(coordinates[i][0], \"'s depth: \", Pixel_Depth)\n",
    "               \n",
    "        cv2.imshow(\"ee\", image_np)  \n",
    "                \n",
    "        cv2.imshow(\"depthh\", image_depth)\n",
    "        #cv2.imshow('KINECT Color', image_np)\n",
    "        \n",
    "        \n",
    "#Action Recognition\n",
    "        max_people = -1\n",
    "        max_depth = -1\n",
    "        if Skeleton_img is not None:\n",
    "\n",
    "            \n",
    "            cv2.resize(Skeleton_img, (640, 480))\n",
    "            print(\"find\", framecheck)            \n",
    "           \n",
    "            feature[1][0] = Kinect.joint_points3D[3].x\n",
    "            feature[1][1] = Kinect.joint_points3D[3].y\n",
    "            feature[1][2] = Kinect.joint_points3D[3].z\n",
    "            \n",
    "            feature[2][0] = Kinect.joint_points3D[2].x\n",
    "            feature[2][1] = Kinect.joint_points3D[2].y\n",
    "            feature[2][2] = Kinect.joint_points3D[2].z\n",
    "            \n",
    "            feature[3][0] = Kinect.joint_points3D[1].x\n",
    "            feature[3][1] = Kinect.joint_points3D[1].y\n",
    "            feature[3][2] = Kinect.joint_points3D[1].z\n",
    "\n",
    "            feature[4][0] = Kinect.joint_points3D[8].x\n",
    "            feature[4][1] = Kinect.joint_points3D[8].y\n",
    "            feature[4][2] = Kinect.joint_points3D[8].z\n",
    "\n",
    "            feature[5][0] = Kinect.joint_points3D[9].x\n",
    "            feature[5][1] = Kinect.joint_points3D[9].y\n",
    "            feature[5][2] = Kinect.joint_points3D[9].z\n",
    "\n",
    "            feature[6][0] = Kinect.joint_points3D[4].x\n",
    "            feature[6][1] = Kinect.joint_points3D[4].y\n",
    "            feature[6][2] = Kinect.joint_points3D[4].z\n",
    "\n",
    "            feature[7][0] = Kinect.joint_points3D[5].x\n",
    "            feature[7][1] = Kinect.joint_points3D[5].y\n",
    "            feature[7][2] = Kinect.joint_points3D[5].z\n",
    "\n",
    "            feature[8][0] = Kinect.joint_points3D[16].x\n",
    "            feature[8][1] = Kinect.joint_points3D[16].y\n",
    "            feature[8][2] = Kinect.joint_points3D[16].z\n",
    "\n",
    "            feature[9][0] = Kinect.joint_points3D[17].x\n",
    "            feature[9][1] = Kinect.joint_points3D[17].y\n",
    "            feature[9][2] = Kinect.joint_points3D[17].z\n",
    "\n",
    "            feature[10][0] = Kinect.joint_points3D[12].x\n",
    "            feature[10][1] = Kinect.joint_points3D[12].y\n",
    "            feature[10][2] = Kinect.joint_points3D[12].z\n",
    "\n",
    "            feature[11][0] = Kinect.joint_points3D[13].x\n",
    "            feature[11][1] = Kinect.joint_points3D[13].y\n",
    "            feature[11][2] = Kinect.joint_points3D[13].z\n",
    "\n",
    "            feature[12][0] = Kinect.joint_points3D[11].x\n",
    "            feature[12][1] = Kinect.joint_points3D[11].y\n",
    "            feature[12][2] = Kinect.joint_points3D[11].z\n",
    "\n",
    "            feature[13][0] = Kinect.joint_points3D[7].x\n",
    "            feature[13][1] = Kinect.joint_points3D[7].y\n",
    "            feature[13][2] = Kinect.joint_points3D[7].z\n",
    "\n",
    "            feature[14][0] = Kinect.joint_points3D[19].x\n",
    "            feature[14][1] = Kinect.joint_points3D[19].y\n",
    "            feature[14][2] = Kinect.joint_points3D[19].z\n",
    "\n",
    "            feature[15][0] = Kinect.joint_points3D[15].x\n",
    "            feature[15][1] = Kinect.joint_points3D[15].y\n",
    "            feature[15][2] = Kinect.joint_points3D[15].z\n",
    "            tmp = []\n",
    "            for i in range (1, 16):\n",
    "                for j in range(3):\n",
    "                    tmp.append(feature[i][j])\n",
    "\n",
    "            frameset = np.concatenate((frameset, tmp), axis = 0)\n",
    "            framecheck+=1\n",
    "            if(framecheck == 30):\n",
    "                frameset = frameset.reshape((framecheck, 45))\n",
    "                for row in range(len(frameset)):\n",
    "                    \n",
    "                    torsox = float(frameset[row][6])\n",
    "                    torsoy = float(frameset[row][7])\n",
    "                    torsoz = float(frameset[row][8])\n",
    "                    neckx = float(frameset[row][3])\n",
    "                    necky = float(frameset[row][4])\n",
    "                    neckz = float(frameset[row][5])\n",
    "                    denom = math.sqrt(math.pow((neckx - torsox),2) + math.pow((necky - torsoy),2) + math.pow((neckz - torsoz),2))\n",
    "                    tmp = []\n",
    "                    i = 0\n",
    "                    while (i < 6):\n",
    "                        tmp.append((float(frameset[row][i]) - torsox)/denom)\n",
    "                        tmp.append((float(frameset[row][i+1]) - torsoy)/denom)\n",
    "                        tmp.append((float(frameset[row][i+2]) - torsoz)/denom)\n",
    "                        i+=3\n",
    "                    i = 9\n",
    "                    while (i < 45):\n",
    "                        tmp.append((float(frameset[row][i]) - torsox)/denom)\n",
    "                        tmp.append((float(frameset[row][i+1]) - torsoy)/denom)\n",
    "                        tmp.append((float(frameset[row][i+2]) - torsoz)/denom)\n",
    "                        i+=3\n",
    "                    norm_frameset = np.concatenate((norm_frameset, tmp), axis = 0)\n",
    "                    \n",
    "                #norm_frameset = norm_frameset.reshape((k, 42))\n",
    "               # CiM,iMjoints,iMaxis = initItemMemories (D, MAXL)\n",
    "                \n",
    "                predictedLabel = hdcResult(AMM,norm_frameset,CiM,iMjoints,iMaxis,MAXL,compareAngle)\n",
    "                print(\"Action Detect----------------------------------------------------------\")\n",
    "                if(predictedLabel == 1):\n",
    "                    action = \"standing\"\n",
    "                    print(\"still\")\n",
    "                elif (predictedLabel == 2 ):\n",
    "                    action = \"sitting\"\n",
    "                    print (\"sit\")\n",
    "                elif (predictedLabel == 3 ):\n",
    "                    action = \"talking on the phone\"\n",
    "                    print (\"phone\")\n",
    "                print(\"-----------------------------------------------------------------------\")\n",
    "               \n",
    "                tmp = []\n",
    "                frameset = np.array([])\n",
    "                norm_frameset = np.array([])\n",
    "                framecheck = 0\n",
    "                if(len(face_names)>= max_people):\n",
    "                    for i in range(len(face_names)): \n",
    "                        people.append( face_names[i])\n",
    "                           \n",
    "                    max_people = len(people)\n",
    "                            \n",
    "       \n",
    "                back_objects = \"\"\n",
    "                front_objects = \"\"\n",
    "                if(flag==1):\n",
    "                    if(len(people)==1): # one person\n",
    "                        if(people[0]):\n",
    "                            situation += people[0] + \" is \"\n",
    "                        else:\n",
    "                            situation += \"Unknown is \"\n",
    "                        situation += action + \" \"\n",
    "                        tmp_situation+=situation\n",
    "                        #back_objects += situation\n",
    "                        #front_objects += situation\n",
    "                        \n",
    "                        if(len(depth_arr)> max_depth):\n",
    "                            real_depth_arr = depth_arr\n",
    "                            print(\"hiiiiiiiiiiiiiiiiiiiiiii\")\n",
    "                            for i in range(len(real_depth_arr)):\n",
    "                                if(real_depth_arr[i][0]==face_names[0] or real_depth_arr[i][0]== \"person\"):\n",
    "                                    index = i\n",
    "                            print(\"index: \", index)\n",
    "                            \n",
    "                            if index==0:\n",
    "                                situation += make_sentence(real_depth_arr, \"behind\", index, 1, len(real_depth_arr)-1 )\n",
    "                            elif index == len(real_depth_arr)-1:\n",
    "                                situation += make_sentence(real_depth_arr, \"in front of\", index, 0, len(real_depth_arr)-2)\n",
    "                            else:\n",
    "                                situation += make_sentence(real_depth_arr, \"in front of\", index, 0, len(real_depth_arr)-2)\n",
    "                                situation += \" and \"\n",
    "                                situation += make_sentence(real_depth_arr, \"behind\", index, 1, len(real_depth_arr)-1 )\n",
    "                    \n",
    "                    elif (len(people)>=2) : # more than two people\n",
    "                        for i in range(len(people)-1):\n",
    "                            situation += people[i] + \" and \"\n",
    "                        situation += people[len(people)-1]+ \" are near by \"\n",
    "                   # if \"person\" in Pixel_Depth_Dict.keys():\n",
    "\n",
    "                        situation += action\n",
    "                    situation += \".\"\n",
    "                    print(\"eng: \", situation)\n",
    "                    word = TextBlob(situation)\n",
    "                    fin_situation = str(word.translate(to='ko'))\n",
    "                    engine.say(fin_situation)\n",
    "                    engine.runAndWait()\n",
    "                    print(\"ko: \", fin_situation)\n",
    "                    tts = gTTS(text=fin_situation, lang='ko')\n",
    "                    tts.save(\"helloKO.mp3\")                    \n",
    "        i=i+1\n",
    "       # print(\"--------------------------------------------------- people: \", people)\n",
    "       # print(\"--------------------------------------------------- situation: \", situation)\n",
    "       # print(\"--------------------------------------------------- face: \", face_names)\n",
    "        \n",
    "                \n",
    "        \n",
    "\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
